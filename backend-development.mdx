---
title: 'Backend Development'
description: 'Learn how to run our servers and make any necessary changes to configurations'
---

<Info>
  **Prerequisite** You should have Python 3 correctly insalled.
</Info>

### The Stack

The Slate api is developed using [FastAPI](https://fastapi.tiangolo.com/). It is hosted on an [AWS EC2](https://aws.amazon.com/ec2/) instance and we use [nginx](https://www.nginx.com/) as a reverse proxy.

### Onboarding Resources

[Creating a custom EC2 instance & hosting FastAPI](https://www.youtube.com/watch?v=SgSnz7kW-Ko&ab_channel=pixegami)  
[ChatGPT Function Calling](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb)

### Running the production server

<Info>
  **Prerequisite** You need to have access to our AWS EC2 instance. Make sure you are in the us-west-2 region.
</Info>

Use this when you want to check real-time traffic or when you need to make changes to the production instance. Before making any changes on the production test them on a seperate EC2 instance and locally first.

Step 1. SSH into the AWS EC2 instance:

```bash
ssh -i "Slate_key.pem" ubuntu@ec2-34-218-116-78.us-west-2.compute.amazonaws.com
```

Step 2. Install the requirements (do this only the first time):

```bash
pip3 install -r requirements.txt
```
Step 3. Start the server locally:

```bash
python3 -m uvicorn main:app
```
By default the server will run on port 8000.

### Persisting the server
<Info>
  Once you have made all desired changes use `tmux` to run the server in the background:
  ```bash
cd Slate-api  
tmux  
python3 -m uvicorn main:app
```
</Info>


### Running the server locally

Use this when testing the server locally after eg. generating new api endpoints or changing existing endpoints.

Step 1. Clone the [GitHUb repo](https://github.com/42enrique/Slate-api)

Step 2. Navigate to the `Slate-api` directory

Step 3. Install the requirements (do this only the first time):

```bash
pip3 install -r requirements.txt
```

Step 4. Install [uvicorn](https://www.uvicorn.org/) (do this only the first time):

```bash
pip3 install uvicorn
```

Step 5. Start the server locally:

```bash
python3 -m uvicorn main:app
```
By default the server will run on port 8000.

### ChatGPT Function Calling

We utilize [ChatGPT function calling](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb) in our api to allow the model to handle more complex user queries such as those linked to reminders.